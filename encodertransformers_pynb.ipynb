{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btknzn/TiagoRobotSkillLearning/blob/main/encodertransformers_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gamDYs-90lgi",
        "outputId": "bb3afa34-b2c6-4dca-dcf8-6cd0c179e5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TqJ2xDwrsO6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.datasets as dset\n",
        "from torch import nn\n",
        "#df = pd.read_csv('PlacingBack/data.csv')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pds\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as dset\n",
        "from torchvision.io import read_image\n",
        "import cv2\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import torchvision.transforms.functional as TF\n",
        "def read_image_dir(image_folder):\n",
        "    image_folder  = image_folder\n",
        "    lst = os.listdir(image_folder) # your directory path\n",
        "    number_files = len(lst)\n",
        "    images = []\n",
        "    for i in range(1,number_files):\n",
        "        folder = image_folder+\"/label\"+str(i)+\".jpg\"\n",
        "        img = cv2.imread(folder)\n",
        "        img.resize([img.shape[2],img.shape[0],img.shape[1]])\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "def read_image_dir_b(image_folder):\n",
        "    image_folder  = image_folder\n",
        "    lst = os.listdir(image_folder) # your directory path\n",
        "    number_files = len(lst)\n",
        "    images = []\n",
        "    for i in range(1,number_files):\n",
        "        folder = image_folder+\"/label\"+str(i)+\".jpg\"\n",
        "        img = cv2.imread(folder,0)\n",
        "        img.resize(1,img.shape[0],img.shape[1])\n",
        "        img = img\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "class MyDataset(Dataset):\n",
        " \n",
        "  def __init__(self,file_name,image_folder_normal,image_folder_depth):\n",
        "    df=pd.read_csv(file_name)\n",
        "    self.image_folder_norma = image_folder_normal\n",
        "    image = read_image_dir(image_folder_normal)\n",
        "    image = torch.asarray(image).float()\n",
        "    #correct it for complex situation\n",
        "    R_means = 188.3424\n",
        "    R_std = 54.67417\n",
        "    G_means = 177.37274\n",
        "    G_std = 66.43382\n",
        "    B_means = 169.08133\n",
        "    B_std = 72.04574\n",
        "    normalized_image = TF.normalize(image,[R_means, G_means, B_means], [R_std, G_std, B_std])\n",
        "    image = torch.asarray(normalized_image).float()\n",
        "    imageDs = read_image_dir_b(image_folder_depth)\n",
        "    D_std =  1.165845\n",
        "    D_means = 1.2093502\n",
        "    imageDs = torch.asarray(imageDs).float()\n",
        "    normalized_disparity = TF.normalize(imageDs,[D_means], [D_std])\n",
        "    imageDs = torch.asarray(normalized_disparity).float()\n",
        "\n",
        "    X1_MIN = 0.04995837\n",
        "    X1_MAX = 0.7562845\n",
        "    arm_joint1 = (df.arm_joint1-X1_MIN)/(X1_MAX-X1_MIN)\n",
        "    X2_MIN = -0.43724102\n",
        "    X2_MAX = 1.0187681\n",
        "    arm_joint2 = (df.arm_joint2-X2_MIN)/(X2_MAX-X2_MIN)\n",
        "    X3_MIN = -3.0051005\n",
        "    X3_MAX = -0.7699633\n",
        "    arm_joint3 = (df.arm_joint3-X3_MIN)/(X3_MAX-X3_MIN)\n",
        "\n",
        "    X4_MIN = 0.5167487\n",
        "    X4_MAX = 2.0607007\n",
        "    arm_joint4 = (df.arm_joint4-X4_MIN)/(X4_MAX-X4_MIN)\n",
        "    X5_MIN = -2.0750659\n",
        "    X5_MAX = 2.0743227\n",
        "    arm_joint5 = (df.arm_joint5-X5_MIN)/(X5_MAX-X5_MIN)\n",
        "\n",
        "    X6_MIN = -1.3956555\n",
        "    X6_MAX = 1.3958309\n",
        "    arm_joint6 = (df.arm_joint6-X6_MIN)/(X6_MAX-X6_MIN)\n",
        "    \n",
        "    X8_MIN = 0.022458013\n",
        "    X8_MAX = 0.045057185\n",
        "    gripper_joint1 = (df.gripper_joint1-X8_MIN)/(X8_MAX-X8_MIN)\n",
        "\n",
        "    X9_MIN = 0.022458013\n",
        "    X9_MAX = 0.045057185\n",
        "    gripper_joint2 = (df.gripper_joint2-X9_MIN)/(X9_MAX-X9_MIN)\n",
        "\n",
        "    X7_MIN = -1.8210393\n",
        "    X7_MAX = 2.0740623\n",
        "    arm_joint7= (df.arm_joint7-X7_MIN)/(X7_MAX-X7_MIN)\n",
        "\n",
        "\n",
        "    X10_MIN = -2.1165963e-06\n",
        "    X10_MAX = 0.3500109\n",
        "    torso_joint = (df.torso_joint-X10_MIN)/(X10_MAX-X10_MIN)\n",
        "\n",
        "    X11_MIN = -0.00033559513\n",
        "    X11_MAX = 0.0006243193\n",
        "    torsohead_joint1 = (df.torso_head1-X11_MIN)/(X11_MAX-X11_MIN)\n",
        "\n",
        "    X12_MIN = -0.7685144\n",
        "    X12_MAX = 0.0014590342  \n",
        "    torsohead_joint2 = (df.torso_head2-X12_MIN)/(X12_MAX-X12_MIN)\n",
        "\n",
        "   \n",
        "\n",
        "   \n",
        "   \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    #1 Feature Number\n",
        "    self.arm_joint1 = arm_joint1.values\n",
        "    #self.arm_joint1 = df.arm_joint1.values\n",
        "    #2 Feature Number\n",
        "    self.arm_joint2 = arm_joint2.values\n",
        "    #self.arm_joint2 = df.arm_joint2.values\n",
        "    #3 Feature Number\n",
        "    self.arm_joint3 = arm_joint3.values\n",
        "    #self.arm_joint3 = df.arm_joint3.values\n",
        "    #4 Feature Number\n",
        "    self.arm_joint4 = arm_joint4.values\n",
        "    #self.arm_joint4 = df.arm_joint4.values\n",
        "    #5 Feature Number\n",
        "    self.arm_joint5 = arm_joint5.values\n",
        "    #self.arm_joint5 = df.arm_joint5.values\n",
        "    #6 Feature Number\n",
        "    self.arm_joint6 = arm_joint6.values\n",
        "    #self.arm_joint6 = df.arm_joint6.values\n",
        "    #7 Feature Number\n",
        "    self.arm_joint7 = arm_joint7.values\n",
        "    #self.arm_joint7 = df.arm_joint7.values\n",
        "    #8 Feature Number\n",
        "    self.gripper_joint1 = gripper_joint1.values\n",
        "    #self.gripper_joint1 = df.gripper_joint1.values\n",
        "    #9 Feature Number\n",
        "    self.gripper_joint2 = gripper_joint2.values\n",
        "    #self.gripper_joint2 = df.gripper_joint2.values\n",
        "    #10 Feature Number\n",
        "    self.torso_joint = torso_joint.values\n",
        "    #self.torso_joint = df.torso_joint.values\n",
        "    #11 Feature Number\n",
        "    self.torsohead_joint1 = torsohead_joint1.values\n",
        "    #self.torsohead_joint1 = df.torso_head1.values\n",
        "    #12 Feature Number\n",
        "    self.torsohead_joint2 = torsohead_joint2.values\n",
        "    #self.torsohead_joint2 = df.torso_head2.values\n",
        "\n",
        "    self.image = image\n",
        "    #self.image = read_image_dir(image_folder_normal)\n",
        "    #self.image =  torch.asarray(self.image).float()\n",
        "    self.imageDs = imageDs\n",
        "    #self.imageDs = read_image_dir_b(image_folder_depth)\n",
        "    #self.imageDs = torch.asarray(self.imageDs).float()\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return min(self.image.shape[0],self.imageDs.shape[0],self.arm_joint6.shape[0],self.torso_joint[0],self.torsohead_joint1[0])-1\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    currentRobot = [self.arm_joint1[idx],self.arm_joint2[idx],self.arm_joint3[idx],self.arm_joint4[idx],self.arm_joint5[idx],self.arm_joint6[idx],self.arm_joint7[idx],self.gripper_joint1[idx],self.gripper_joint2[idx],self.torso_joint[idx],self.torsohead_joint1[idx],self.torsohead_joint2[idx]]\n",
        "    if idx >= len(self.arm_joint1)-2:   \n",
        "      nextActionRobot = [self.arm_joint1[idx],self.arm_joint2[idx],self.arm_joint3[idx],self.arm_joint4[idx],self.arm_joint5[idx],self.arm_joint6[idx],self.arm_joint7[idx],self.gripper_joint1[idx],self.gripper_joint2[idx],self.torso_joint[idx],self.torsohead_joint1[idx],self.torsohead_joint2[idx]]\n",
        "      x = torch.Tensor(currentRobot)\n",
        "      x = x.reshape((1,x.shape[0]))\n",
        "      image_current = self.image[idx]\n",
        "      image_next = self.image[idx]\n",
        "      depth_current = self.imageDs[idx]\n",
        "      depth_next = self.imageDs[idx]\n",
        "      y = torch.Tensor(nextActionRobot)\n",
        "      y = y.reshape((1,y.shape[0]))\n",
        "      return x,y,image_current,image_next,depth_current,depth_next\n",
        "\n",
        "    else:\n",
        "      image_current = self.image[idx] \n",
        "      depth_current = self.imageDs[idx]\n",
        "      idx = idx+1\n",
        "      nextActionRobot = [self.arm_joint1[idx],self.arm_joint2[idx],self.arm_joint3[idx],self.arm_joint4[idx],self.arm_joint5[idx],self.arm_joint6[idx],self.arm_joint7[idx],self.gripper_joint1[idx],self.gripper_joint2[idx],self.torso_joint[idx],self.torsohead_joint1[idx],self.torsohead_joint2[idx]]\n",
        "      x = torch.Tensor(currentRobot)\n",
        "      x = x.reshape((1,x.shape[0]))\n",
        "      y = torch.Tensor(nextActionRobot)\n",
        "      y = y.reshape((1,y.shape[0]))\n",
        "      image_next = self.image[idx]\n",
        "      depth_next = self.imageDs[idx]\n",
        "      return x,y,image_current,image_next,depth_current,depth_next\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQz3QoKLwmQj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.datasets as dset\n",
        "from torch import nn\n",
        "#df = pd.read_csv('PlacingBack/data.csv')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pds\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader,IterableDataset\n",
        "import torchvision.datasets as dset\n",
        "from torchvision.io import read_image\n",
        "import cv2\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "\n",
        "\n",
        "class DatasetItar(IterableDataset):\n",
        "  def __init__(self,file_name,image_folder_normal,image_folder_depth):\n",
        "    self.data = MyDataset(file_name,image_folder_normal,image_folder_depth)\n",
        "  def __iter__(self):\n",
        "        return iter(self.data)\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rppe_XPARD5g"
      },
      "outputs": [],
      "source": [
        "class autoencoderBLACK(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(autoencoderBLACK, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=5,stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.Conv2d(8,16,kernel_size=5,stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Conv2d(16,32,kernel_size=5,stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32,16,kernel_size=5,stride=2),\n",
        "            nn.ReLU(),\n",
        "            ) \n",
        "        \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ConvTranspose2d(16,32,kernel_size=5,stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ConvTranspose2d(32,16,kernel_size=5,stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ConvTranspose2d(16,8,kernel_size=5,stride=2,output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ConvTranspose2d(8,1,kernel_size=5,stride=2,output_padding=1),\n",
        "            nn.Sigmoid(),\n",
        "            nn.BatchNorm2d(1)\n",
        "            )    \n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJAAQ0i6wqY0",
        "outputId": "8d5fe03d-4201-452f-f0df-86ad93e8de25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTHBl6WlzzYG",
        "outputId": "82a100e8-2be0-4ddd-c823-101c4b990415"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "autoencoderBLACK = autoencoderBLACK()\n",
        "\n",
        "autoencoderBLACK.load_state_dict(torch.load(\"/content/drive/MyDrive/datasets/AEBLACK.pth\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGL4DE7_wuwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952b0653-aab9-470e-a04a-0844ce2128e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import  torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as dset\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.utils as vutils\n",
        "from torchsummary import summary\n",
        "from IPython import display\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, models, transforms\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as dset\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.utils as vutils\n",
        "from torchsummary import summary\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import transformers\n",
        "#from transformers import TransformerEncoder, TransformerDecoder\n",
        "encoder_resnet = torchvision.models.resnet50(pretrained=True)\n",
        "class TransformerPredictor(nn.Module):\n",
        "    def __init__(self, n_hidden=1024, future=0):\n",
        "        super(TransformerPredictor, self).__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.future = future\n",
        "        self.max_seq_len = 2300  # maximum sequence length\n",
        "        self.d_model = 2300  # hidden size\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(self.d_model, 4, self.d_model),\n",
        "            num_layers=6,\n",
        "            norm=nn.LayerNorm(self.d_model)\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(self.d_model, 4, self.d_model),\n",
        "            num_layers=6,\n",
        "            norm=nn.LayerNorm(self.d_model)\n",
        "        )\n",
        "        self.linear = nn.Linear(self.d_model, 12)\n",
        "        \n",
        "        self.encoder_RGB= torch.nn.Sequential(*(list(encoder_resnet.children())[:-1]))\n",
        "        self.encoder_D = autoencoderBLACK.encoder\n",
        "\n",
        "\n",
        "    def forward(self, x, image_current, depth_current):\n",
        "        # Encode the image and depth data\n",
        "        latent_RGB = self.encoder_RGB(image_current)\n",
        "        latent_D = self.encoder_D(depth_current)\n",
        "        # Flatten the data and concatenate them\n",
        "        flatten_x = x.flatten(start_dim=1)\n",
        "        flatten_RGB = latent_RGB.flatten(start_dim=1)\n",
        "        flatten_D = latent_D.flatten(start_dim=1)\n",
        "        concat_input = torch.cat((flatten_x, flatten_RGB, flatten_D), dim=1)\n",
        "        # Encode the concatenated input\n",
        "        pos_encoder_input = concat_input\n",
        "        encoded_input = self.encoder(pos_encoder_input)\n",
        "        # Decode the encoded input\n",
        "        decoded_output = self.decoder(encoded_input, encoded_input)\n",
        "        # Flatten the decoded output and apply the linear layer\n",
        "        flatten_output = decoded_output.flatten(start_dim=1)\n",
        "        output = self.linear(flatten_output)\n",
        "        # Return the output\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKpFptaww-23",
        "outputId": "35e83434-af06-47a7-d9f5-0c06d6f67004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-f82e967b5693>:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  image = torch.asarray(image).float()\n"
          ]
        }
      ],
      "source": [
        "datasets_partial = []\n",
        "data_folder = '/content/drive/MyDrive/datasets'\n",
        "lst = os.listdir(data_folder)\n",
        "number_files = len(lst)\n",
        "completeMotionFolder = []\n",
        "for i in range(0,number_files):\n",
        "  folderCompleteMotion = str(i+1)\n",
        "  if  folderCompleteMotion in lst:\n",
        "    completeMotionFolder.append(data_folder+\"/\"+folderCompleteMotion)\n",
        "          \n",
        "      \n",
        "datasets = []            \n",
        "for i in range(0,len(completeMotionFolder)):\n",
        "  folder = completeMotionFolder[i]\n",
        "  lst = os.listdir(folder)\n",
        "  zeroCSV = folder +\"/0.csv\"\n",
        "  depthZERO = folder+'/depth/0'\n",
        "  imageZERO = folder + '/Image/0'\n",
        "  dataset = [zeroCSV,imageZERO,depthZERO] \n",
        "  datasets.append(dataset)\n",
        "\n",
        "train_loaders = []\n",
        "for i in range(0,len(datasets)):\n",
        "  CSV = datasets[i][0]\n",
        "  depth = datasets[i][1]\n",
        "  image = datasets[i][2]\n",
        "  dataset = DatasetItar(CSV,depth,image)\n",
        "  train_loader = DataLoader(dataset,shuffle=False,batch_size=32)\n",
        "  train_loaders.append(train_loader) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1AK8DpuxBxL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "model = TransformerPredictor().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5K1k4EuMxZ5v",
        "outputId": "fd8f2a0d-7101-40ed-f575-f375a01debbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32, 1, 12])) that is different to the input size (torch.Size([32, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([4, 1, 12])) that is different to the input size (torch.Size([4, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([17, 1, 12])) that is different to the input size (torch.Size([17, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([7, 1, 12])) that is different to the input size (torch.Size([7, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([9, 1, 12])) that is different to the input size (torch.Size([9, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([8, 1, 12])) that is different to the input size (torch.Size([8, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([27, 1, 12])) that is different to the input size (torch.Size([27, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([29, 1, 12])) that is different to the input size (torch.Size([29, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([13, 1, 12])) that is different to the input size (torch.Size([13, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([3, 1, 12])) that is different to the input size (torch.Size([3, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.epochnumberloss: tensor(0.0922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "1.epochnumberloss: tensor(0.0911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "2.epochnumberloss: tensor(0.0909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "3.epochnumberloss: tensor(0.0915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "4.epochnumberloss: tensor(0.0882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "5.epochnumberloss: tensor(0.0863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "6.epochnumberloss: tensor(0.0906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "7.epochnumberloss: tensor(0.0920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "8.epochnumberloss: tensor(0.0936, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "9.epochnumberloss: tensor(0.0908, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "10.epochnumberloss: tensor(0.0896, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "11.epochnumberloss: tensor(0.0919, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "12.epochnumberloss: tensor(0.0882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "13.epochnumberloss: tensor(0.0903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "14.epochnumberloss: tensor(0.0911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "15.epochnumberloss: tensor(0.0883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "16.epochnumberloss: tensor(0.0872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "17.epochnumberloss: tensor(0.0921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "18.epochnumberloss: tensor(0.0898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "19.epochnumberloss: tensor(0.0869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "20.epochnumberloss: tensor(0.0874, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "21.epochnumberloss: tensor(0.0888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "22.epochnumberloss: tensor(0.0883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "23.epochnumberloss: tensor(0.0892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "24.epochnumberloss: tensor(0.0872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "25.epochnumberloss: tensor(0.0906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "26.epochnumberloss: tensor(0.0893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "27.epochnumberloss: tensor(0.0880, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "28.epochnumberloss: tensor(0.0903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "29.epochnumberloss: tensor(0.0883, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-39d51663556e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_current\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdepth_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e36db1429536>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, image_current, depth_current)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Encode the concatenated input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mpos_encoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_encoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Decode the encoded input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mdecoded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    548\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                            need_weights=False)[0]\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_VF.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_epoch = 1000\n",
        "model.encoder_D.requires_grad_ = True\n",
        "model.encoder_RGB.requires_grad_=True\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-7\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "for epoch in range(0,n_epoch):\n",
        "  currentLoss = []\n",
        "  for j in range(0,len(train_loaders)):\n",
        "    train_loader = train_loaders[j]\n",
        "    for data in (train_loader):\n",
        "      x = data[0][:,:,:].to(device)\n",
        "      x_next = data[1][:,:,:].to(device)\n",
        "      image_current = data[2].to(device)\n",
        "      image_next = data[3].to(device)\n",
        "      depth_current = data[4].to(device)\n",
        "      depth_next = data[5].to(device)\n",
        "      train_input = x\n",
        "      train_target = x_next\n",
        "      optimizer.zero_grad()\n",
        "      out = model(train_input,image_current,depth_current)\n",
        "      loss = criterion(out,train_target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      currentLoss.append(loss)\n",
        "  print(str(epoch)+\".epochnumber\"+ \"loss:\", sum(currentLoss)/len(currentLoss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70j1j2LDRhUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef16489c-b0a0-4d39-d01c-46d9386b4ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1, 12])) that is different to the input size (torch.Size([1, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(0.2857, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.2195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.2077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1231, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.1003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0886, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0828, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0800, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "loss: tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "n_epoch = 1000\n",
        "model.encoder_D.requires_grad_=True\n",
        "model.encoder_RGB.requires_grad_=True\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-7\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "for epoch in range(0,n_epoch):\n",
        "  currentLoss = []\n",
        "  for j in range(0,len(train_loaders)):\n",
        "    train_loader = train_loaders[j]\n",
        "    for data in (train_loader):\n",
        "      x = data[0][:,:,:].to(device)\n",
        "      x_next = data[1][:,:,:].to(device)\n",
        "      image_current = data[2].to(device)\n",
        "      image_next = data[3].to(device)\n",
        "      depth_current = data[4].to(device)\n",
        "      depth_next = data[5].to(device)\n",
        "      train_input = x\n",
        "      train_target = x_next\n",
        "      optimizer.zero_grad()\n",
        "      out = model(train_input,image_current,depth_current)\n",
        "      loss = criterion(out,train_target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      currentLoss.append(loss)\n",
        "      print(\"loss:\", sum(currentLoss)/len(currentLoss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdkvjWq7SOIx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "d7c36726-3806-4f03-ccd2-17f81ada7360"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6af85c4e83a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'or'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs/x1axis.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2119\u001b[0;31m                 result = print_method(\n\u001b[0m\u001b[1;32m   2120\u001b[0m                     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m                     \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m                 _png.write_png(renderer._renderer, fh, self.figure.dpi,\n\u001b[1;32m    537\u001b[0m                                metadata={**default_metadata, **metadata})\n",
            "\u001b[0;32m/usr/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outputs/x1axis.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x1 = []\n",
        "x1_pred = []\n",
        "x2 = []\n",
        "x2_pred = []\n",
        "x3 = []\n",
        "x3_pred = []\n",
        "x4 = []\n",
        "x4_pred = []\n",
        "x5 = []\n",
        "x5_pred = []\n",
        "x6 = []\n",
        "x6_pred = []\n",
        "x7 = []\n",
        "x7_pred = []\n",
        "x8 = []\n",
        "x8_pred = []\n",
        "x9= []\n",
        "x9_pred = []\n",
        "x10 = []\n",
        "x10_pred = []\n",
        "x11 = []\n",
        "x11_pred = []\n",
        "x12 = []\n",
        "x12_pred = []\n",
        "\n",
        "\n",
        "train_loaders = []\n",
        "for i in range(0,len(datasets)):\n",
        "  CSV = datasets[i][0]\n",
        "  depth = datasets[i][1]\n",
        "  image = datasets[i][2]\n",
        "  dataset = DatasetItar(CSV,depth,image)\n",
        "  train_loader = DataLoader(dataset,shuffle=False,batch_size=1)\n",
        "  train_loaders.append(train_loader) \n",
        "\n",
        "for j in range(0,len(train_loaders)):\n",
        "  train_loader = train_loaders[j]\n",
        "  for data in (train_loader):\n",
        "    x = data[0][:,:,:].to(device)\n",
        "    x_next = data[1][:,:,:].to(device)\n",
        "    image_current = data[2].to(device)\n",
        "    image_next = data[3].to(device)\n",
        "    depth_current = data[4].to(device)\n",
        "    depth_next = data[5].to(device)\n",
        "    train_input = x\n",
        "    train_target = x_next\n",
        "    out = model(train_input,image_current,depth_current)\n",
        "    x1.append(torch.flatten(x[:,:,0]).tolist())\n",
        "    x2.append(torch.flatten(x[:,:,1]).tolist())\n",
        "    x3.append(torch.flatten(x[:,:,2]).tolist())\n",
        "    x4.append(torch.flatten(x[:,:,3]).tolist())\n",
        "    x5.append(torch.flatten(x[:,:,4]).tolist())\n",
        "    x6.append(torch.flatten(x[:,:,5]).tolist())\n",
        "    x7.append(torch.flatten(x[:,:,6]).tolist())\n",
        "    x8.append(torch.flatten(x[:,:,7]).tolist())\n",
        "    x9.append(torch.flatten(x[:,:,8]).tolist())\n",
        "    x10.append(torch.flatten(x[:,:,9]).tolist())\n",
        "    x11.append(torch.flatten(x[:,:,10]).tolist())\n",
        "    x12.append(torch.flatten(x[:,:,11]).tolist())\n",
        "    x1_pred.append(out[:,0].tolist())\n",
        "    x2_pred.append(out[:,1].tolist())\n",
        "    x3_pred.append(out[:,2].tolist())\n",
        "    x4_pred.append(out[:,3].tolist())\n",
        "    x5_pred.append(out[:,4].tolist())\n",
        "    x6_pred.append(out[:,5].tolist())\n",
        "    x7_pred.append(out[:,6].tolist())\n",
        "    x8_pred.append(out[:,7].tolist())\n",
        "    x9_pred.append(out[:,8].tolist())\n",
        "    x10_pred.append(out[:,9].tolist())\n",
        "    x11_pred.append(out[:,10].tolist())\n",
        "    x12_pred.append(out[:,11].tolist())\n",
        "                \n",
        "                \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "x1 = np.array(x1)\n",
        "x2 = np.array(x2)\n",
        "x3 = np.array(x3)\n",
        "x4 = np.array(x4)\n",
        "x5 = np.array(x5)\n",
        "x6 = np.array(x6)\n",
        "x7 = np.array(x7)\n",
        "x8 = np.array(x8)\n",
        "x9 = np.array(x9)\n",
        "x10 = np.array(x10)\n",
        "x11 = np.array(x11)\n",
        "x12 = np.array(x12)\n",
        "\n",
        "x1_pred = np.array(x1_pred)\n",
        "x2_pred = np.array(x2_pred)\n",
        "x3_pred = np.array(x3_pred)\n",
        "x4_pred = np.array(x4_pred)\n",
        "x5_pred = np.array(x5_pred)\n",
        "x6_pred = np.array(x6_pred)\n",
        "x7_pred = np.array(x7_pred)\n",
        "x8_pred = np.array(x8_pred)\n",
        "x9_pred = np.array(x9_pred)\n",
        "x10_pred = np.array(x10_pred)\n",
        "x11_pred = np.array(x11_pred)\n",
        "x12_pred = np.array(x12_pred)\n",
        "\n",
        "\n",
        "t1 = np.arange(0.0, x1_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t1, x1, 'bo')\n",
        "plt.plot( t1, x1_pred, 'or')\n",
        "plt.savefig('outputs/x1axis.png')\n",
        "\n",
        "t2 = np.arange(0.0, x2_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t2, x2, 'bo')   \n",
        "plt.plot( t2, x2_pred, 'or')\n",
        "plt.savefig('outputs/x2axis.png')\n",
        "\n",
        "t3 = np.arange(0.0, x3_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t3, x3, 'bo')\n",
        "plt.plot( t3, x3_pred, 'or')\n",
        "plt.savefig('outputs/x1axis.png')\n",
        "\n",
        "\n",
        "t4 = np.arange(0.0, x4_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t4, x4, 'bo')\n",
        "plt.plot( t4, x4_pred, 'or')\n",
        "plt.savefig('outputs/x4axis.png')\n",
        "\n",
        "t5 = np.arange(0.0, x5_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t5, x5, 'bo')\n",
        "plt.plot( t5, x5_pred, 'or')\n",
        "plt.savefig('outputs/x5axis.png')\n",
        "\n",
        "t6 = np.arange(0.0, x6_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t6, x6, 'bo')\n",
        "plt.plot( t6, x6_pred, 'or')\n",
        "plt.savefig('outputs/x6axis.png')\n",
        "\n",
        "t7 = np.arange(0.0, x7_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t7, x7, 'bo')\n",
        "plt.plot( t7, x7_pred, 'or')\n",
        "plt.savefig('outputs/x7axis.png')   \n",
        "\n",
        "t8 = np.arange(0.0, x8_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t8, x8, 'bo')\n",
        "plt.plot( t8, x8_pred, 'or')\n",
        "plt.savefig('outputs/x8axis.png') \n",
        "\n",
        "t9 = np.arange(0.0, x9_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t9, x9, 'bo')\n",
        "plt.plot( t9, x9_pred, 'or')\n",
        "plt.savefig('outputs/x9axis.png') \n",
        "\n",
        "\n",
        "t10 = np.arange(0.0, x10_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t10, x10, 'bo')\n",
        "plt.plot( t10, x10_pred, 'or')\n",
        "plt.savefig('outputs/x10axis.png') \n",
        "\n",
        "t11 = np.arange(0.0, x11_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t11, x11, 'bo')\n",
        "plt.plot( t11, x11_pred, 'or')\n",
        "plt.savefig('outputs/x11axis.png') \n",
        "\n",
        "t12 = np.arange(0.0, x12_pred.shape[0], 1) \n",
        "plt.figure()\n",
        "plt.plot(t12, x12, 'bo')\n",
        "plt.plot( t12, x12_pred, 'or')\n",
        "plt.savefig('outputs/x12axis.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUmq7ljQSQU7"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/datasets/transformers1.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOZ7nlBju9/sS+tjKHM+ncC",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}